{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10513963,"sourceType":"datasetVersion","datasetId":6508017}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:44:46.554198Z","iopub.execute_input":"2025-01-19T09:44:46.554531Z","iopub.status.idle":"2025-01-19T09:44:47.836085Z","shell.execute_reply.started":"2025-01-19T09:44:46.554497Z","shell.execute_reply":"2025-01-19T09:44:47.834912Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/word-duplicates/new_df.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"new_df= pd.read_csv('/kaggle/input/word-duplicates/new_df.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:44:47.837162Z","iopub.execute_input":"2025-01-19T09:44:47.837717Z","iopub.status.idle":"2025-01-19T09:44:48.176892Z","shell.execute_reply.started":"2025-01-19T09:44:47.837674Z","shell.execute_reply":"2025-01-19T09:44:48.175849Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"new_df.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:44:59.898579Z","iopub.execute_input":"2025-01-19T09:44:59.898959Z","iopub.status.idle":"2025-01-19T09:44:59.905993Z","shell.execute_reply.started":"2025-01-19T09:44:59.898926Z","shell.execute_reply":"2025-01-19T09:44:59.904825Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(49606, 18)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"new_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:45:15.687009Z","iopub.execute_input":"2025-01-19T09:45:15.687385Z","iopub.status.idle":"2025-01-19T09:45:15.716747Z","shell.execute_reply.started":"2025-01-19T09:45:15.687355Z","shell.execute_reply":"2025-01-19T09:45:15.715589Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                          question1  \\\n0      398782  best marketing automation tool small mid size ...   \n1      115086                                   poor want invest   \n2      327711   india live abroad met guy france party want date   \n3      367788                  many people u hate southern state   \n4      151235                     consequence bhopal gas tragedy   \n\n                                           question2  is_duplicate  q1_len  \\\n0  best marketing automation tool small mid sized...             1      53   \n1                               quite poor want rich             0      16   \n2  e thapar university thapar university institut...             0      48   \n3  boyfriend doesnt feel guilty hurt cried tellin...             0      33   \n4                   reason behind bhopal gas tragedy             0      30   \n\n   q2_len  q1_num_words  q2_num_words  word_common  word_share   cwc_min  \\\n0      54             8             8            7        0.44  0.874989   \n1      20             3             4            2        0.29  0.666644   \n2      90             9            11            0        0.00  0.000000   \n3      81             6            12            0        0.00  0.000000   \n4      32             4             5            3        0.33  0.749981   \n\n    cwc_max  last_word_eq  first_word_eq  fuzz_ratio  fuzz_partial_ratio  \\\n0  0.874989           1.0            1.0          99                  98   \n1  0.499988           0.0            0.0          61                  73   \n2  0.000000           0.0            0.0          26                  40   \n3  0.000000           0.0            0.0          16                  31   \n4  0.599988           1.0            0.0          74                  79   \n\n   token_sort_ratio  token_set_ratio  \n0                99               99  \n1                50               72  \n2                30               30  \n3                26               26  \n4                58               75  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>word_common</th>\n      <th>word_share</th>\n      <th>cwc_min</th>\n      <th>cwc_max</th>\n      <th>last_word_eq</th>\n      <th>first_word_eq</th>\n      <th>fuzz_ratio</th>\n      <th>fuzz_partial_ratio</th>\n      <th>token_sort_ratio</th>\n      <th>token_set_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>398782</td>\n      <td>best marketing automation tool small mid size ...</td>\n      <td>best marketing automation tool small mid sized...</td>\n      <td>1</td>\n      <td>53</td>\n      <td>54</td>\n      <td>8</td>\n      <td>8</td>\n      <td>7</td>\n      <td>0.44</td>\n      <td>0.874989</td>\n      <td>0.874989</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>99</td>\n      <td>98</td>\n      <td>99</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>115086</td>\n      <td>poor want invest</td>\n      <td>quite poor want rich</td>\n      <td>0</td>\n      <td>16</td>\n      <td>20</td>\n      <td>3</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0.29</td>\n      <td>0.666644</td>\n      <td>0.499988</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>61</td>\n      <td>73</td>\n      <td>50</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>327711</td>\n      <td>india live abroad met guy france party want date</td>\n      <td>e thapar university thapar university institut...</td>\n      <td>0</td>\n      <td>48</td>\n      <td>90</td>\n      <td>9</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>26</td>\n      <td>40</td>\n      <td>30</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>367788</td>\n      <td>many people u hate southern state</td>\n      <td>boyfriend doesnt feel guilty hurt cried tellin...</td>\n      <td>0</td>\n      <td>33</td>\n      <td>81</td>\n      <td>6</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>16</td>\n      <td>31</td>\n      <td>26</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>151235</td>\n      <td>consequence bhopal gas tragedy</td>\n      <td>reason behind bhopal gas tragedy</td>\n      <td>0</td>\n      <td>30</td>\n      <td>32</td>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0.33</td>\n      <td>0.749981</td>\n      <td>0.599988</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>74</td>\n      <td>79</td>\n      <td>58</td>\n      <td>75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"new_df.drop('Unnamed: 0', axis = 1, inplace =True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:45:46.551984Z","iopub.execute_input":"2025-01-19T09:45:46.552318Z","iopub.status.idle":"2025-01-19T09:45:46.566438Z","shell.execute_reply.started":"2025-01-19T09:45:46.552293Z","shell.execute_reply":"2025-01-19T09:45:46.565242Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"new_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:45:54.451356Z","iopub.execute_input":"2025-01-19T09:45:54.451675Z","iopub.status.idle":"2025-01-19T09:45:54.457676Z","shell.execute_reply.started":"2025-01-19T09:45:54.451650Z","shell.execute_reply":"2025-01-19T09:45:54.456520Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(49606, 17)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"ques_df = new_df[['question1','question2']]\nques_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:46:09.646565Z","iopub.execute_input":"2025-01-19T09:46:09.647104Z","iopub.status.idle":"2025-01-19T09:46:09.659501Z","shell.execute_reply.started":"2025-01-19T09:46:09.647072Z","shell.execute_reply":"2025-01-19T09:46:09.658486Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                           question1  \\\n0  best marketing automation tool small mid size ...   \n1                                   poor want invest   \n2   india live abroad met guy france party want date   \n3                  many people u hate southern state   \n4                     consequence bhopal gas tragedy   \n\n                                           question2  \n0  best marketing automation tool small mid sized...  \n1                               quite poor want rich  \n2  e thapar university thapar university institut...  \n3  boyfriend doesnt feel guilty hurt cried tellin...  \n4                   reason behind bhopal gas tragedy  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question1</th>\n      <th>question2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>best marketing automation tool small mid size ...</td>\n      <td>best marketing automation tool small mid sized...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>poor want invest</td>\n      <td>quite poor want rich</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>india live abroad met guy france party want date</td>\n      <td>e thapar university thapar university institut...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>many people u hate southern state</td>\n      <td>boyfriend doesnt feel guilty hurt cried tellin...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>consequence bhopal gas tragedy</td>\n      <td>reason behind bhopal gas tragedy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"final_df = new_df.drop(columns=['question1','question2'])\nprint(final_df.shape)\nfinal_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:46:15.619972Z","iopub.execute_input":"2025-01-19T09:46:15.620295Z","iopub.status.idle":"2025-01-19T09:46:15.641675Z","shell.execute_reply.started":"2025-01-19T09:46:15.620272Z","shell.execute_reply":"2025-01-19T09:46:15.640477Z"}},"outputs":[{"name":"stdout","text":"(49606, 15)\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   is_duplicate  q1_len  q2_len  q1_num_words  q2_num_words  word_common  \\\n0             1      53      54             8             8            7   \n1             0      16      20             3             4            2   \n2             0      48      90             9            11            0   \n3             0      33      81             6            12            0   \n4             0      30      32             4             5            3   \n\n   word_share   cwc_min   cwc_max  last_word_eq  first_word_eq  fuzz_ratio  \\\n0        0.44  0.874989  0.874989           1.0            1.0          99   \n1        0.29  0.666644  0.499988           0.0            0.0          61   \n2        0.00  0.000000  0.000000           0.0            0.0          26   \n3        0.00  0.000000  0.000000           0.0            0.0          16   \n4        0.33  0.749981  0.599988           1.0            0.0          74   \n\n   fuzz_partial_ratio  token_sort_ratio  token_set_ratio  \n0                  98                99               99  \n1                  73                50               72  \n2                  40                30               30  \n3                  31                26               26  \n4                  79                58               75  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>word_common</th>\n      <th>word_share</th>\n      <th>cwc_min</th>\n      <th>cwc_max</th>\n      <th>last_word_eq</th>\n      <th>first_word_eq</th>\n      <th>fuzz_ratio</th>\n      <th>fuzz_partial_ratio</th>\n      <th>token_sort_ratio</th>\n      <th>token_set_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>53</td>\n      <td>54</td>\n      <td>8</td>\n      <td>8</td>\n      <td>7</td>\n      <td>0.44</td>\n      <td>0.874989</td>\n      <td>0.874989</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>99</td>\n      <td>98</td>\n      <td>99</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>16</td>\n      <td>20</td>\n      <td>3</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0.29</td>\n      <td>0.666644</td>\n      <td>0.499988</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>61</td>\n      <td>73</td>\n      <td>50</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>48</td>\n      <td>90</td>\n      <td>9</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>26</td>\n      <td>40</td>\n      <td>30</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>33</td>\n      <td>81</td>\n      <td>6</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>16</td>\n      <td>31</td>\n      <td>26</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>30</td>\n      <td>32</td>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0.33</td>\n      <td>0.749981</td>\n      <td>0.599988</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>74</td>\n      <td>79</td>\n      <td>58</td>\n      <td>75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import gensim\nfrom gensim.models import Word2Vec\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# Ensure you have the necessary NLTK packages\nnltk.download('punkt')\n\n# Replace NaN or non-string values with empty strings in question1 and question2\nques_df['question1'] = ques_df['question1'].fillna(\"\").astype(str)\nques_df['question2'] = ques_df['question2'].fillna(\"\").astype(str)\n\n# Combine questions for Word2Vec training\nquestions = list(ques_df['question1']) + list(ques_df['question2'])\ntokenized_questions = [word_tokenize(question.lower()) for question in questions]\n\n# Train the Word2Vec model\nmodel = Word2Vec(sentences=tokenized_questions, vector_size=100, window=5, min_count=1, sg=0)\n\n# Define a function to get the average word vector for each question\ndef get_average_word_vector(sentence, model, vector_size):\n    if not sentence:  # Handle empty or invalid sentences\n        return np.zeros(vector_size)\n    words = word_tokenize(sentence.lower())\n    word_vectors = [model.wv[word] for word in words if word in model.wv]\n    \n    if len(word_vectors) == 0:  # If no word vectors are found, return a zero vector\n        return np.zeros(vector_size)\n    \n    return np.mean(word_vectors, axis=0)\n\n# Get the average word vectors for each question\nq1_arr = np.array([get_average_word_vector(question, model, 100) for question in ques_df['question1']])\nq2_arr = np.array([get_average_word_vector(question, model, 100) for question in ques_df['question2']])\n\n# Verify the shapes of q1_arr and q2_arr\nprint(\"q1_arr shape:\", q1_arr.shape)\nprint(\"q2_arr shape:\", q2_arr.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:52:11.043147Z","iopub.execute_input":"2025-01-19T09:52:11.043491Z","iopub.status.idle":"2025-01-19T09:52:38.340032Z","shell.execute_reply.started":"2025-01-19T09:52:11.043465Z","shell.execute_reply":"2025-01-19T09:52:38.338695Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-12-2a4c8201bff2>:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ques_df['question1'] = ques_df['question1'].fillna(\"\").astype(str)\n<ipython-input-12-2a4c8201bff2>:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ques_df['question2'] = ques_df['question2'].fillna(\"\").astype(str)\n","output_type":"stream"},{"name":"stdout","text":"q1_arr shape: (49606, 100)\nq2_arr shape: (49606, 100)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"temp_df1 = pd.DataFrame(q1_arr, index= ques_df.index)\ntemp_df2 = pd.DataFrame(q2_arr, index= ques_df.index)\ntemp_df = pd.concat([temp_df1, temp_df2], axis=1)\ntemp_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:53:53.825594Z","iopub.execute_input":"2025-01-19T09:53:53.825976Z","iopub.status.idle":"2025-01-19T09:53:53.907771Z","shell.execute_reply.started":"2025-01-19T09:53:53.825945Z","shell.execute_reply":"2025-01-19T09:53:53.906547Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(49606, 200)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"final_df = pd.concat([final_df, temp_df], axis=1)\nprint(final_df.shape)\nfinal_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:54:00.923913Z","iopub.execute_input":"2025-01-19T09:54:00.924253Z","iopub.status.idle":"2025-01-19T09:54:01.074135Z","shell.execute_reply.started":"2025-01-19T09:54:00.924225Z","shell.execute_reply":"2025-01-19T09:54:01.073173Z"}},"outputs":[{"name":"stdout","text":"(49606, 215)\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   is_duplicate  q1_len  q2_len  q1_num_words  q2_num_words  word_common  \\\n0             1      53      54             8             8            7   \n1             0      16      20             3             4            2   \n2             0      48      90             9            11            0   \n3             0      33      81             6            12            0   \n4             0      30      32             4             5            3   \n\n   word_share   cwc_min   cwc_max  last_word_eq  ...        90        91  \\\n0        0.44  0.874989  0.874989           1.0  ...  0.812994 -0.047199   \n1        0.29  0.666644  0.499988           0.0  ...  0.693783  0.236019   \n2        0.00  0.000000  0.000000           0.0  ...  0.653823  0.136772   \n3        0.00  0.000000  0.000000           0.0  ...  0.250175  0.297643   \n4        0.33  0.749981  0.599988           1.0  ...  0.161569  0.081845   \n\n         92        93        94        95        96        97        98  \\\n0  0.192553 -0.063288  0.902283  0.226069  0.087011 -0.295854  0.122438   \n1 -0.010697  0.251330  0.895667  0.074056  0.094465 -0.288312 -0.352310   \n2  0.760982  0.000940  1.272126 -0.030455  0.580520 -0.168829  0.509723   \n3 -0.004345  0.258051  0.502317  0.421697  0.136965 -0.390650 -0.199890   \n4  0.135822  0.223044  0.587892  0.017106  0.309029 -0.119430 -0.033150   \n\n         99  \n0 -0.132236  \n1  0.022238  \n2  0.110468  \n3  0.130210  \n4  0.176809  \n\n[5 rows x 215 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>word_common</th>\n      <th>word_share</th>\n      <th>cwc_min</th>\n      <th>cwc_max</th>\n      <th>last_word_eq</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>53</td>\n      <td>54</td>\n      <td>8</td>\n      <td>8</td>\n      <td>7</td>\n      <td>0.44</td>\n      <td>0.874989</td>\n      <td>0.874989</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.812994</td>\n      <td>-0.047199</td>\n      <td>0.192553</td>\n      <td>-0.063288</td>\n      <td>0.902283</td>\n      <td>0.226069</td>\n      <td>0.087011</td>\n      <td>-0.295854</td>\n      <td>0.122438</td>\n      <td>-0.132236</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>16</td>\n      <td>20</td>\n      <td>3</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0.29</td>\n      <td>0.666644</td>\n      <td>0.499988</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.693783</td>\n      <td>0.236019</td>\n      <td>-0.010697</td>\n      <td>0.251330</td>\n      <td>0.895667</td>\n      <td>0.074056</td>\n      <td>0.094465</td>\n      <td>-0.288312</td>\n      <td>-0.352310</td>\n      <td>0.022238</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>48</td>\n      <td>90</td>\n      <td>9</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.653823</td>\n      <td>0.136772</td>\n      <td>0.760982</td>\n      <td>0.000940</td>\n      <td>1.272126</td>\n      <td>-0.030455</td>\n      <td>0.580520</td>\n      <td>-0.168829</td>\n      <td>0.509723</td>\n      <td>0.110468</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>33</td>\n      <td>81</td>\n      <td>6</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.250175</td>\n      <td>0.297643</td>\n      <td>-0.004345</td>\n      <td>0.258051</td>\n      <td>0.502317</td>\n      <td>0.421697</td>\n      <td>0.136965</td>\n      <td>-0.390650</td>\n      <td>-0.199890</td>\n      <td>0.130210</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>30</td>\n      <td>32</td>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0.33</td>\n      <td>0.749981</td>\n      <td>0.599988</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.161569</td>\n      <td>0.081845</td>\n      <td>0.135822</td>\n      <td>0.223044</td>\n      <td>0.587892</td>\n      <td>0.017106</td>\n      <td>0.309029</td>\n      <td>-0.119430</td>\n      <td>-0.033150</td>\n      <td>0.176809</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 215 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(final_df.iloc[:,1:].values,final_df.iloc[:,0].values,test_size=0.2,random_state=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:54:56.042968Z","iopub.execute_input":"2025-01-19T09:54:56.043352Z","iopub.status.idle":"2025-01-19T09:54:56.239622Z","shell.execute_reply.started":"2025-01-19T09:54:56.043321Z","shell.execute_reply":"2025-01-19T09:54:56.238453Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\naccuracy_score(y_test,y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:55:03.672890Z","iopub.execute_input":"2025-01-19T09:55:03.673227Z","iopub.status.idle":"2025-01-19T09:56:28.991211Z","shell.execute_reply.started":"2025-01-19T09:55:03.673200Z","shell.execute_reply":"2025-01-19T09:56:28.990034Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0.7914734932473292"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\ny_pred1 = xgb.predict(X_test)\naccuracy_score(y_test,y_pred1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T09:57:39.765764Z","iopub.execute_input":"2025-01-19T09:57:39.766220Z","iopub.status.idle":"2025-01-19T09:57:46.350099Z","shell.execute_reply.started":"2025-01-19T09:57:39.766191Z","shell.execute_reply":"2025-01-19T09:57:46.349312Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0.7827050997782705"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [10, 20, 30],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2],\n    'max_features': ['sqrt', 'log2']\n}\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n                           cv=3, verbose=2, n_jobs=-1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\n\n# Best hyperparameters\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\n\n# Evaluate the model\nbest_rf = grid_search.best_estimator_\ny_pred = best_rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Test Accuracy:\", accuracy)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T10:05:32.456139Z","iopub.execute_input":"2025-01-19T10:05:32.456489Z","iopub.status.idle":"2025-01-19T11:35:38.089066Z","shell.execute_reply.started":"2025-01-19T10:05:32.456464Z","shell.execute_reply":"2025-01-19T11:35:38.087792Z"}},"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 72 candidates, totalling 216 fits\nBest Hyperparameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\nTest Accuracy: 0.7905664180608748\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.optimizers import Adam\n\n# Define the neural network model\nmodel = Sequential()\n\n# Input layer (assumed the number of features is 10)\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n\n# Hidden layers\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\n\n# Output layer (binary classification)\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T11:46:58.018463Z","iopub.execute_input":"2025-01-19T11:46:58.018872Z","iopub.status.idle":"2025-01-19T11:47:38.994339Z","shell.execute_reply.started":"2025-01-19T11:46:58.018834Z","shell.execute_reply":"2025-01-19T11:47:38.993157Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m1241/1241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6739 - loss: 0.6037 - val_accuracy: 0.7353 - val_loss: 0.4969\nEpoch 2/10\n\u001b[1m1241/1241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7196 - loss: 0.5148 - val_accuracy: 0.7451 - val_loss: 0.4869\nEpoch 3/10\n\u001b[1m1241/1241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7392 - loss: 0.4892 - val_accuracy: 0.7457 - val_loss: 0.4814\nEpoch 4/10\n\u001b[1m1241/1241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7408 - loss: 0.4850 - val_accuracy: 0.7514 - val_loss: 0.4714\nEpoch 5/10\n\u001b[1m1241/1241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7476 - loss: 0.4747 - val_accuracy: 0.7532 - val_loss: 0.4650\nEpoch 6/10\n\u001b[1m1241/1241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7502 - loss: 0.4722 - val_accuracy: 0.7475 - val_loss: 0.4732\nEpoch 7/10\n\u001b[1m1241/1241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.4681 - val_accuracy: 0.7652 - val_loss: 0.4497\nEpoch 8/10\n\u001b[1m1241/1241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7575 - loss: 0.4652 - val_accuracy: 0.7681 - val_loss: 0.4529\nEpoch 9/10\n\u001b[1m1241/1241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4561 - val_accuracy: 0.7650 - val_loss: 0.4514\nEpoch 10/10\n\u001b[1m1241/1241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7633 - loss: 0.4546 - val_accuracy: 0.7632 - val_loss: 0.4536\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b3a9b61ff70>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Make predictions on the test set\ny_pred_prob = model.predict(X_test)\n\n# Convert probabilities to binary predictions\ny_pred = (y_pred_prob > 0.5).astype(int)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T11:51:58.424362Z","iopub.execute_input":"2025-01-19T11:51:58.425200Z","iopub.status.idle":"2025-01-19T11:51:59.133972Z","shell.execute_reply.started":"2025-01-19T11:51:58.425165Z","shell.execute_reply":"2025-01-19T11:51:59.132683Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\nAccuracy: 0.7632\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}